import pandas as pd
import json
import time
import random
from bs4 import BeautifulSoup
from pathlib import Path
import os

# å¼•å…¥Seleniumç›¸å…³æ¨¡å—
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException 

# --- æ ¸å¿ƒé…ç½® ---
SEARCH_URL = "https://we.51job.com/pc/search"

# è¯·åœ¨è¿™é‡Œå¡«å…¥ä½ ä»æµè§ˆå™¨ä¸­è·å¾—çš„æœ€æ–°çš„Cookieå­—ç¬¦ä¸²
COOKIES_STR = '' # ä¾‹å¦‚: 'acw_tc=...; ssxmod_itp=...;'

def parse_cookies(cookies_str):
    if not cookies_str: return []
    cookies_list = []
    for item in cookies_str.split(';'):
        if '=' in item:
            name, value = item.strip().split('=', 1)
            cookies_list.append({'name': name, 'value': value, 'domain': '.51job.com'})
    return cookies_list

def get_job_info_with_selenium(keyword, pages):
    """
    ä½¿ç”¨Seleniumé©±åŠ¨çœŸå®æµè§ˆå™¨ï¼Œã€é€šè¿‡æ¨¡æ‹Ÿç‚¹å‡»'ä¸‹ä¸€é¡µ'æŒ‰é’®ã€‘æ¥ç¿»é¡µã€‚
    """
    
    # --- 1. åˆå§‹åŒ– WebDriver ---
    print("æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨é©±åŠ¨...")
    options = webdriver.ChromeOptions()
    # options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36')
    
    driver_path = os.path.join(os.path.dirname(__file__), '..', 'drivers', 'chromedriver.exe')
    
    try:
        service = ChromeService(executable_path=driver_path)
        driver = webdriver.Chrome(service=service, options=options)
    except Exception as e:
        print(f"æµè§ˆå™¨é©±åŠ¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return pd.DataFrame()
    print("æµè§ˆå™¨é©±åŠ¨åˆå§‹åŒ–æˆåŠŸã€‚")

    # --- 2. å‡†å¤‡å·¥ä½œï¼šè®¿é—®ä¸»é¡µï¼Œæ·»åŠ Cookie ---
    driver.get("https://we.51job.com/") 
    cookies = parse_cookies(COOKIES_STR)
    if not cookies:
        print("ã€è­¦å‘Šã€‘Cookieä¸ºç©ºï¼")
    else:
        for cookie in cookies:
            driver.add_cookie(cookie)
        print("Cookieæ·»åŠ æˆåŠŸã€‚")

    # --- 3. é¦–æ¬¡è®¿é—®æœç´¢ç»“æœçš„ç¬¬ä¸€é¡µ ---
    print("æ­£åœ¨åŠ è½½æœç´¢ç»“æœç¬¬ä¸€é¡µ...")
    target_url = f"{SEARCH_URL}?keyword={keyword}&jobArea=000000&pageNum=1"
    driver.get(target_url)
    
    all_jobs_data = []

    # --- 4. å¾ªç¯ç‚¹å‡»'ä¸‹ä¸€é¡µ'æŒ‰é’® ---
    for page in range(1, pages + 1):
        print(f"--- æ­£åœ¨å¤„ç†ç¬¬ {page} é¡µæ•°æ® ---")

        try:
            # ç­‰å¾…èŒä½åˆ—è¡¨åŠ è½½å®Œæˆ
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.CLASS_NAME, "joblist-item-job"))
            )
            print("é¡µé¢èŒä½åˆ—è¡¨åŠ è½½æˆåŠŸï¼")
            time.sleep(random.uniform(1.5, 2.5)) # ç­‰å¾…JSæ¸²æŸ“

            # æå–å½“å‰é¡µé¢çš„æ•°æ®
            html_source = driver.page_source
            soup = BeautifulSoup(html_source, 'lxml')
            job_cards = soup.find_all('div', class_='joblist-item-job')

            if not job_cards:
                print(f"è­¦å‘Šï¼šç¬¬ {page} é¡µæœªè§£æå‡ºèŒä½å¡ç‰‡ï¼Œå¯èƒ½ä¸ºç©ºç™½é¡µã€‚")
                continue # ç»§ç»­å°è¯•ä¸‹ä¸€é¡µ

            page_job_count = 0
            for card in job_cards:
                if 'sensorsdata' in card.attrs:
                    try:
                        job_data = json.loads(card['sensorsdata'])
                        all_jobs_data.append(job_data)
                        page_job_count += 1
                    except json.JSONDecodeError:
                        pass
            
            print(f"ç¬¬ {page} é¡µæˆåŠŸè§£æåˆ° {page_job_count} æ¡æ–°èŒä½ã€‚")

            # ã€æ ¸å¿ƒç¿»é¡µæ­¥éª¤ã€‘æŸ¥æ‰¾å¹¶ç‚¹å‡»â€œä¸‹ä¸€é¡µâ€æŒ‰é’®
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€é¡µ (å¦‚æœ"ä¸‹ä¸€é¡µ"æŒ‰é’®æ˜¯ç°è‰²ä¸å¯ç‚¹å‡»çŠ¶æ€)
            next_button = driver.find_element(By.CLASS_NAME, "btn-next")
            if "is-disabled" in next_button.get_attribute("class"):
                print("æ£€æµ‹åˆ°'ä¸‹ä¸€é¡µ'æŒ‰é’®å·²ç¦ç”¨ï¼Œå·²åˆ°è¾¾æœ€åä¸€é¡µã€‚æŠ“å–ç»“æŸã€‚")
                break
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€é¡µï¼Œåˆ™ç‚¹å‡»æŒ‰é’®
            next_button.click()
            print("å·²ç‚¹å‡»ã€ä¸‹ä¸€é¡µã€‘ï¼Œç­‰å¾…é¡µé¢åˆ·æ–°...")

        except TimeoutException:
            print(f"é”™è¯¯ï¼šåœ¨ç¬¬ {page} é¡µç­‰å¾…èŒä½åˆ—è¡¨è¶…æ—¶ï¼Œå¯èƒ½å·²è¢«æ‹¦æˆªæˆ–ç½‘ç»œé—®é¢˜ã€‚")
            break
        except NoSuchElementException:
            print(f"é”™è¯¯ï¼šåœ¨ç¬¬ {page} é¡µæœªæ‰¾åˆ°'ä¸‹ä¸€é¡µ'æŒ‰é’®ï¼ŒæŠ“å–ç»“æŸã€‚")
            break
        except Exception as e:
            print(f"æŠ“å–ç¬¬ {page} é¡µæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            break
            
    # --- 5. å…³é—­æµè§ˆå™¨ ---
    driver.quit()
    print("æµè§ˆå™¨å·²å…³é—­ã€‚")

    if not all_jobs_data:
        return pd.DataFrame()
    
    # å› ä¸ºæ•°æ®æ˜¯ç´¯åŠ çš„ï¼Œæœ€åå†ç»Ÿä¸€åˆ›å»ºDataFrameå’Œå»é‡
    df = pd.DataFrame(all_jobs_data)
    print(f"\nå»é‡å‰ï¼Œå…±æŠ“å–åˆ° {len(df)} æ¡èŒä½è®°å½•ã€‚")
    df.drop_duplicates(subset=['jobId'], inplace=True)
    print(f"åŸºäº'jobId'å»é‡åï¼Œå‰©ä½™ {len(df)} æ¡ç‹¬ä¸€æ— äºŒçš„èŒä½ã€‚")
    
    required_columns = ['jobId', 'jobTitle', 'jobArea', 'jobSalary', 'jobYear', 'jobDegree','companyId', 'companyName', 'companyTypeString', 'companySizeString','jobLabel', 'jobHref']
    existing_columns = [col for col in required_columns if col in df.columns]
    return df[existing_columns]

if __name__ == '__main__':
    # --- 1. å‚æ•°è®¾å®š ---
    search_keyword = 'Python'
    total_pages = 50 # è®¾å®šç›®æ ‡æŠ“å–50é¡µ
    
    # --- 2. æ‰§è¡Œæ ¸å¿ƒçˆ¬è™«é€»è¾‘ ---
    print(f"ğŸš€ å¼€å§‹æŠ“å–å…³é”®è¯ä¸º '{search_keyword}' çš„èŒä½ä¿¡æ¯ï¼Œç›®æ ‡é¡µæ•°ï¼š{total_pages}...")
    df_raw = get_job_info_with_selenium(search_keyword, total_pages)
    
    # --- 3. æ•°æ®å¤„ç†ä¸ä¿å­˜ ---
    if not df_raw.empty:
        # æ„å»ºç»å¯¹ã€å¯é çš„è·¯å¾„
        BASE_DIR = Path(__file__).resolve().parent.parent # .parent.parent ä» scraper ç›®å½•è¿”å›åˆ°é¡¹ç›®æ ¹ç›®å½•
        DATA_DIR = BASE_DIR / 'data'
        
        # åœ¨å†™å…¥æ–‡ä»¶å‰ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨
        DATA_DIR.mkdir(parents=True, exist_ok=True)
        
        # å®šä¹‰æœ€ç»ˆè¾“å‡ºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        output_path = DATA_DIR / 'jobs_raw.csv'
        
        # å®‰å…¨åœ°ä¿å­˜æ–‡ä»¶
        df_raw.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        # --- 4. ä»»åŠ¡æ€»ç»“ ---
        print(f"\nâœ… ã€æˆåŠŸã€‘æ•°æ®æŠ“å–å®Œæˆï¼")
        print(f"ğŸ“ˆ æœ€ç»ˆè·å¾— {len(df_raw)} æ¡æœ‰æ•ˆèŒä½æ•°æ®ã€‚")
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³: {output_path}")
    else:
        print("\nâš ï¸ ã€ä»»åŠ¡ç»“æŸã€‘æœªèƒ½è·å–ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–çˆ¬è™«é€»è¾‘ã€‚")